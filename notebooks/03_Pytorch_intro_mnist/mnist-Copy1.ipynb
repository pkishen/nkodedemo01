{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch with Kubeflow Fairing \n",
    "\n",
    "In this notebook we will walk through training a character recongition model using the MNIST dataset on Pytorch. \n",
    "We will then show you how to use Kubeflow Fairing to run the same training job on both Kubeflow and CMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already up-to-date: torchvision in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 1)) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 1)) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision->-r requirements.txt (line 2)) (8.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#you can skip this step if you have already installed the necessary dependencies\n",
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_model.py\n",
    "import argparse\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "# For mac users you may get hit with this bug https://github.com/pytorch/pytorch/issues/20030\n",
    "# temporary solution is \"brew install libomp\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Model Defintion\n",
    "\n",
    "Setup a Convolution Nueral network using Pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_model.py -a\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Training and Test Functions\n",
    "A simple training function that batches the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_model.py -a\n",
    "def train(model, device, train_loader, optimizer, epoch, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and batch_idx>0:\n",
    "            print('Train Epoch: {}\\t[{}/{}\\t({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "              100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_model.py -a\n",
    "def test(model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(\n",
    "              output, target, size_average=False).item()  # sum up batch loss\n",
    "            pred = output.max(\n",
    "              1, keepdim=True)[1]  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "          test_loss, correct, len(test_loader.dataset),\n",
    "          100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to train_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_model.py -a\n",
    "def nkTrain(batch_size=64, epochs=1, log_interval=100, lr=0.01, model_dir=None, momentum=0.5, \n",
    "                       no_cuda=False, seed=1, test_batch_size=1000):\n",
    "\n",
    "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "    print(\"Using {} for training.\".format(device))\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "      datasets.MNIST(\n",
    "          'data',\n",
    "          train=True,\n",
    "          download=True,\n",
    "          transform=transforms.Compose([\n",
    "              transforms.ToTensor(),\n",
    "              # Normalize a tensor image with mean and standard deviation\n",
    "              transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "          ])),\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "      datasets.MNIST(\n",
    "          'data',\n",
    "          train=False,\n",
    "          transform=transforms.Compose([\n",
    "              transforms.ToTensor(),\n",
    "              # Normalize a tensor image with mean and standard deviation              \n",
    "              transforms.Normalize(mean=(0.1307,), std=(0.3081,))\n",
    "          ])),\n",
    "      batch_size=test_batch_size,\n",
    "      shuffle=True,\n",
    "      **kwargs)\n",
    "\n",
    "    model = Net().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        start_time = time.time()\n",
    "        train(model, device, train_loader, optimizer, epoch, log_interval)\n",
    "        print(\"Time taken for epoch #{}: {:.2f}s\".format(epoch, time.time()-start_time))\n",
    "        test(model, device, test_loader, epoch)\n",
    "\n",
    "    if model_dir:\n",
    "        model_file_name = 'torch.model'\n",
    "        tmp_model_file = os.path.join('/tmp', model_file_name)\n",
    "        torch.save(model.state_dict(), tmp_model_file)\n",
    "        subprocess.check_call([\n",
    "            'gsutil', 'cp', tmp_model_file,\n",
    "            os.path.join(model_dir, model_file_name)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for training.\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.1%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "113.5%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "180.4%/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 1\t[6400/60000\t(11%)]\tLoss: 1.929761\n",
      "Train Epoch: 1\t[12800/60000\t(21%)]\tLoss: 1.327502\n",
      "Train Epoch: 1\t[19200/60000\t(32%)]\tLoss: 0.846595\n",
      "Train Epoch: 1\t[25600/60000\t(43%)]\tLoss: 0.674760\n",
      "Train Epoch: 1\t[32000/60000\t(53%)]\tLoss: 0.442683\n",
      "Train Epoch: 1\t[38400/60000\t(64%)]\tLoss: 0.704966\n",
      "Train Epoch: 1\t[44800/60000\t(75%)]\tLoss: 0.470975\n",
      "Train Epoch: 1\t[51200/60000\t(85%)]\tLoss: 0.810192\n",
      "Train Epoch: 1\t[57600/60000\t(96%)]\tLoss: 0.412998\n",
      "Time taken for epoch #1: 19.26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2063, Accuracy: 9387/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nkTrain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Docker Images and train using the container.\n",
    "\n",
    "In this block we set some Docker config. Fairing will use this information to package up the `train_and_test` function i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to train job setup...\n",
      "\u001b[33m[W 201020 02:37:39 function:49]\u001b[m The FunctionPreProcessor is optimized for using in a notebook or IPython environment. For it to work, the python version should be same for both local python and the python in the docker. Please look at alternatives like BasePreprocessor or FullNotebookPreprocessor.\n",
      "\u001b[33m[W 201020 02:37:39 tasks:62]\u001b[m Using builder: <class 'kubeflow.fairing.builders.cluster.cluster.ClusterBuilder'>\n",
      "about to submit job\n",
      "\u001b[32m[I 201020 02:37:39 tasks:66]\u001b[m Building the docker image.\n",
      "\u001b[32m[I 201020 02:37:39 cluster:46]\u001b[m Building image using cluster builder.\n",
      "\u001b[33m[W 201020 02:37:39 base:94]\u001b[m /usr/local/lib/python3.6/dist-packages/kubeflow/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "\u001b[32m[I 201020 02:37:39 base:107]\u001b[m Creating docker context: /tmp/fairing_context_7606uv9l\n",
      "\u001b[33m[W 201020 02:37:39 base:94]\u001b[m /usr/local/lib/python3.6/dist-packages/kubeflow/fairing/__init__.py already exists in Fairing context, skipping...\n",
      "\u001b[33m[W 201020 02:37:39 aws:70]\u001b[m Not able to find aws credentials secret: aws-secret\n",
      "\u001b[33m[W 201020 02:37:39 manager:298]\u001b[m Waiting for fairing-builder-x54mh-lzbcp to start...\n",
      "\u001b[33m[W 201020 02:37:39 manager:298]\u001b[m Waiting for fairing-builder-x54mh-lzbcp to start...\n",
      "\u001b[33m[W 201020 02:37:39 manager:298]\u001b[m Waiting for fairing-builder-x54mh-lzbcp to start...\n",
      "\u001b[32m[I 201020 02:37:41 manager:304]\u001b[m Pod started running True\n",
      "\u001b[36mINFO\u001b[0m[0000] Retrieving image manifest tensorflow/tensorflow:1.15.0-py3\n",
      "\u001b[36mINFO\u001b[0m[0001] Retrieving image manifest tensorflow/tensorflow:1.15.0-py3\n",
      "\u001b[36mINFO\u001b[0m[0002] Built cross stage deps: map[]\n",
      "\u001b[36mINFO\u001b[0m[0002] Retrieving image manifest tensorflow/tensorflow:1.15.0-py3\n",
      "\u001b[36mINFO\u001b[0m[0002] Retrieving image manifest tensorflow/tensorflow:1.15.0-py3\n",
      "\u001b[36mINFO\u001b[0m[0003] Executing 0 build triggers\n",
      "\u001b[36mINFO\u001b[0m[0003] Unpacking rootfs as cmd COPY /app//requirements.txt /app/ requires it.\n",
      "\u001b[36mINFO\u001b[0m[0036] WORKDIR /app/\n",
      "\u001b[36mINFO\u001b[0m[0036] cmd: workdir\n",
      "\u001b[36mINFO\u001b[0m[0036] Changed working directory to /app/\n",
      "\u001b[36mINFO\u001b[0m[0036] Creating directory /app/\n",
      "\u001b[36mINFO\u001b[0m[0036] Resolving 1 paths\n",
      "\u001b[36mINFO\u001b[0m[0036] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0036] ENV FAIRING_RUNTIME 1\n",
      "\u001b[36mINFO\u001b[0m[0036] COPY /app//requirements.txt /app/\n",
      "\u001b[36mINFO\u001b[0m[0036] Resolving 1 paths\n",
      "\u001b[36mINFO\u001b[0m[0036] Taking snapshot of files...\n",
      "\u001b[36mINFO\u001b[0m[0036] RUN if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi\n",
      "\u001b[36mINFO\u001b[0m[0036] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0037] Resolving 27381 paths\n",
      "\u001b[36mINFO\u001b[0m[0041] cmd: /bin/sh\n",
      "\u001b[36mINFO\u001b[0m[0041] args: [-c if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi]\n",
      "\u001b[36mINFO\u001b[0m[0041] Running: [/bin/sh -c if [ -e requirements.txt ];then pip install --no-cache -r requirements.txt; fi]\n",
      "Collecting torch\n",
      "  Downloading https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
      "Collecting torchvision\n",
      "  Downloading https://files.pythonhosted.org/packages/8e/dc/4a939cfbd38398f4765f712576df21425241020bfccc200af76d19088533/torchvision-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (5.9MB)\n",
      "Collecting future\n",
      "  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->-r requirements.txt (line 1)) (1.17.3)\n",
      "Collecting pillow>=4.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/a6/9c/829c74f7f7f129616dfb6f75afd72529f895d29db71c7aeb46b02fcfb26d/Pillow-8.0.0-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py): started\n",
      "  Building wheel for future (setup.py): finished with status 'done'\n",
      "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=493275 sha256=c96ab3050231054a59ab43f820e2530585fea71f8d7559cd55a18bfe7d2500d2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ify5vkxb/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
      "Successfully built future\n",
      "Installing collected packages: future, torch, pillow, torchvision\n",
      "Successfully installed future-0.18.2 pillow-8.0.0 torch-1.6.0 torchvision-0.7.0\n",
      "WARNING: You are using pip version 19.3.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[36mINFO\u001b[0m[0071] Taking snapshot of full filesystem...\n",
      "\u001b[36mINFO\u001b[0m[0078] Resolving 32300 paths\n",
      "\u001b[36mINFO\u001b[0m[0115] COPY /app/ /app/\n",
      "\u001b[36mINFO\u001b[0m[0115] Resolving 12 paths\n",
      "\u001b[36mINFO\u001b[0m[0115] Taking snapshot of files...\n",
      "\u001b[33m[W 201020 02:41:07 aws:70]\u001b[m Not able to find aws credentials secret: aws-secret\n",
      "\u001b[33m[W 201020 02:41:07 job:101]\u001b[m The job fairing-job-59mmv launched.\n",
      "\u001b[33m[W 201020 02:41:07 manager:298]\u001b[m Waiting for fairing-job-59mmv-7f7ml to start...\n",
      "\u001b[33m[W 201020 02:41:07 manager:298]\u001b[m Waiting for fairing-job-59mmv-7f7ml to start...\n",
      "\u001b[33m[W 201020 02:41:07 manager:298]\u001b[m Waiting for fairing-job-59mmv-7f7ml to start...\n",
      "\u001b[32m[I 201020 02:41:39 manager:304]\u001b[m Pod started running True\n",
      "Using cpu for training.\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 1\t[6400/60000\t(11%)]\tLoss: 1.929761\n",
      "Train Epoch: 1\t[12800/60000\t(21%)]\tLoss: 1.327502\n",
      "Train Epoch: 1\t[19200/60000\t(32%)]\tLoss: 0.846595\n",
      "Train Epoch: 1\t[25600/60000\t(43%)]\tLoss: 0.674760\n",
      "Train Epoch: 1\t[32000/60000\t(53%)]\tLoss: 0.442683\n",
      "Train Epoch: 1\t[38400/60000\t(64%)]\tLoss: 0.704966\n",
      "Train Epoch: 1\t[44800/60000\t(75%)]\tLoss: 0.470975\n",
      "Train Epoch: 1\t[51200/60000\t(85%)]\tLoss: 0.810192\n",
      "Train Epoch: 1\t[57600/60000\t(96%)]\tLoss: 0.412998\n",
      "Time taken for epoch #1: 18.86s\n",
      "\n",
      "Test set: Average loss: 0.2063, Accuracy: 9387/10000 (94%)\n",
      "\n",
      "15.7%\n",
      "29.8%\n",
      "43.9%\n",
      "58.0%\n",
      "72.1%\n",
      "86.2%\n",
      "56.4%%\n",
      "85.5%%\n",
      "180.4%/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:469: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n",
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "\u001b[33m[W 201020 02:42:07 job:173]\u001b[m Cleaning up job fairing-job-59mmv...\n"
     ]
    }
   ],
   "source": [
    "!sh ~/nkodedemo01/nkode/remote_train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
